{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input : 3 inputs, d_model=4\n",
      "x: [[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
    "x = np.array([[1.0, 0.0, 1.0, 0.0], [0.0, 2.0, 0.0, 2.0], [1.0, 1.0, 1.0, 1.0]])\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: weights 3 dimensions x d_model=4\n",
      "w_query: [[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n",
      "w_key: [[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n",
      "w_value: [[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
    "w_query = np.array([[1, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 1]])\n",
    "print(\"w_query:\", w_query)\n",
    "\n",
    "w_key = np.array([[0, 0, 1], [1, 1, 0], [0, 1, 0], [1, 1, 0]])\n",
    "print(\"w_key:\", w_key)\n",
    "\n",
    "w_value = np.array([[0, 2, 0], [0, 3, 0], [1, 0, 3], [1, 1, 0]])\n",
    "print(\"w_value:\", w_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Matrix multiplication to obtain Q,K,V\n",
      "Query: x * w_query\n",
      "Q: [[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n",
      "Key: x * w_key\n",
      "K: [[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n",
      "Value: x * w_value\n",
      "V: [[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
    "print(\"Query: x * w_query\")\n",
    "Q = np.matmul(x, w_query)\n",
    "print(\"Q:\", Q)\n",
    "\n",
    "print(\"Key: x * w_key\")\n",
    "K = np.matmul(x, w_key)\n",
    "print(\"K:\", K)\n",
    "\n",
    "print(\"Value: x * w_value\")\n",
    "V = np.matmul(x, w_value)\n",
    "print(\"V:\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Scaled Attention Scores\n",
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Scaled Attention Scores\")\n",
    "k_d = 1  # Equation is normally the square root of the number of dimensions (3 in this case)\n",
    "attention_scores = (Q @ K.transpose()) / k_d\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Scaled softmax attention_scores for each vector\n",
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
    "attention_scores[0] = softmax(attention_scores[0])\n",
    "attention_scores[1] = softmax(attention_scores[1])\n",
    "attention_scores[2] = softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: attention value obtained by score1/k_d * V\n",
      "[1. 2. 3.]\n",
      "[2. 8. 0.]\n",
      "[2. 6. 3.]\n",
      "Attention 1: [0.06337894 0.12675788 0.19013681]\n",
      "Attention 2: [0.93662106 3.74648425 0.        ]\n",
      "Attention 3: [0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
    "print(V[0])\n",
    "print(V[1])\n",
    "print(V[2])\n",
    "attention1 = attention_scores[0].reshape(-1, 1)\n",
    "attention1 = attention_scores[0][0] * V[0]\n",
    "print(\"Attention 1:\", attention1)\n",
    "\n",
    "attention2 = attention_scores[0][1] * V[1]\n",
    "print(\"Attention 2:\", attention2)\n",
    "\n",
    "attention3 = attention_scores[0][2] * V[2]\n",
    "print(\"Attention 3:\", attention3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: summed the results to create the first line of the output matrix\n",
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: summed the results to create the first line of the output matrix\")\n",
    "attention_input1 = attention1 + attention2 + attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: Step 1 to 7 for inputs 1 to 3\n",
      "[[0.58751243 0.32876733 0.25570795 0.80796168 0.00177578 0.3998109\n",
      "  0.34918701 0.83385879 0.34601251 0.40699334 0.5755632  0.74275413\n",
      "  0.99623933 0.38535444 0.78002393 0.60278954 0.03993106 0.66794002\n",
      "  0.20279803 0.74728954 0.53061567 0.1623918  0.60425018 0.36700845\n",
      "  0.90030672 0.34287176 0.49342534 0.68111791 0.52227084 0.23022488\n",
      "  0.60751024 0.29777488 0.19143023 0.86031908 0.95459437 0.34278289\n",
      "  0.70680581 0.44989966 0.92446136 0.64225194 0.16512493 0.30836662\n",
      "  0.18318507 0.65111996 0.10952488 0.99216561 0.4160243  0.35574822\n",
      "  0.58277061 0.6588462  0.16634101 0.93591091 0.20831083 0.94459796\n",
      "  0.83201022 0.91816635 0.85229539 0.55188509 0.87808955 0.25804807\n",
      "  0.43139655 0.41685581 0.3131693  0.31489676]\n",
      " [0.0894747  0.22128897 0.30873792 0.02885298 0.02613985 0.36090353\n",
      "  0.71915251 0.60197054 0.44946902 0.04960437 0.97914378 0.26214332\n",
      "  0.21773916 0.05149666 0.3046916  0.1319246  0.24183341 0.687092\n",
      "  0.24055382 0.27891963 0.37645041 0.4829427  0.83286317 0.73106781\n",
      "  0.8880818  0.32601943 0.51945728 0.25064702 0.72214254 0.76331393\n",
      "  0.77343285 0.93595556 0.84747325 0.84014213 0.37511973 0.94396688\n",
      "  0.05427488 0.60744492 0.53525306 0.57818958 0.50410678 0.26014546\n",
      "  0.15410664 0.64606258 0.58909471 0.82855793 0.89395675 0.56575652\n",
      "  0.60059219 0.22637111 0.27738185 0.37757636 0.66703852 0.1786765\n",
      "  0.99743459 0.44406475 0.77917053 0.16530677 0.72318226 0.81071603\n",
      "  0.2855698  0.97732886 0.47743418 0.37881607]\n",
      " [0.34467297 0.94104754 0.3975311  0.87943209 0.7316697  0.25146152\n",
      "  0.37456912 0.03655597 0.51321721 0.05701785 0.43006244 0.14950811\n",
      "  0.25502371 0.70730817 0.88090481 0.28474694 0.37477854 0.18588442\n",
      "  0.39295005 0.69480546 0.21544131 0.35584412 0.72514916 0.78544183\n",
      "  0.65425578 0.34475418 0.11709418 0.05787326 0.22896368 0.31069116\n",
      "  0.0209062  0.4788901  0.56820927 0.17196469 0.83037499 0.00884671\n",
      "  0.46420029 0.04341405 0.93289263 0.72033893 0.87049695 0.20671899\n",
      "  0.24848935 0.22060855 0.98031585 0.44899295 0.08739253 0.63552747\n",
      "  0.85656913 0.61903219 0.31644261 0.46508076 0.57963927 0.23109836\n",
      "  0.36432641 0.36905452 0.61829383 0.71057293 0.74963462 0.14344099\n",
      "  0.44718446 0.88719645 0.26274197 0.90554636]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
    "# This is assuming that we had actually gone through the whole process for all 3\n",
    "# We'll just take a random matrix of the correct dimensions in lieu\n",
    "attention_head1 = np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
      "shape of one head (3, 64) dimension of 8 heads 512\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
    "z0h1 = np.random.random((3, 64))\n",
    "z1h2 = np.random.random((3, 64))\n",
    "z2h3 = np.random.random((3, 64))\n",
    "z3h4 = np.random.random((3, 64))\n",
    "z4h5 = np.random.random((3, 64))\n",
    "z5h6 = np.random.random((3, 64))\n",
    "z6h7 = np.random.random((3, 64))\n",
    "z7h8 = np.random.random((3, 64))\n",
    "print(\"shape of one head\", z0h1.shape, \"dimension of 8 heads\", 64 * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
      "[[0.86927409 0.12836694 0.59222786 ... 0.62225058 0.76491867 0.38528535]\n",
      " [0.32556618 0.39026433 0.42130743 ... 0.59319731 0.0660897  0.04835302]\n",
      " [0.78720993 0.6180019  0.22714473 ... 0.32089113 0.53065514 0.78705635]]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\"\n",
    ")\n",
    "output_attention = np.hstack((z0h1, z1h2, z2h3, z3h4, z4h5, z5h6, z6h7, z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask, scale=True):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (numpy.ndarray): array of query representations with shape (L_q by d)\n",
    "        key (numpy.ndarray): array of key representations with shape (L_k by d)\n",
    "        value (numpy.ndarray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (numpy.ndarray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "        scale (bool): whether to scale the dot product of the query and transposed key\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        query.shape[-1] == key.shape[-1] == value.shape[-1]\n",
    "    ), \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    if scale:\n",
    "        depth = query.shape[-1]\n",
    "    else:\n",
    "        depth = 1\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = np.matmul(query, np.swapaxes(key, -1, -2)) / np.sqrt(depth)\n",
    "\n",
    "    # Apply the mask\n",
    "    if mask is not None:\n",
    "        dots = np.where(mask, dots, np.full_like(dots, -1e9))\n",
    "\n",
    "    # Softmax formula implementation\n",
    "    # Use scipy.special.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
    "    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = scipy.special.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use np.exp()\n",
    "    dots = np.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use np.matmul()\n",
    "    attention = np.matmul(dots, value)\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_dot_product_self_attention(q, k, v, scale=True):\n",
    "    \"\"\"Masked dot product self attention.\n",
    "    Args:\n",
    "        q (numpy.ndarray): queries.\n",
    "        k (numpy.ndarray): keys.\n",
    "        v (numpy.ndarray): values.\n",
    "    Returns:\n",
    "        numpy.ndarray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Size of the penultimate dimension of the query\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Use np.tril() - Lower triangle of an array and np.ones()\n",
    "    mask = np.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)\n",
    "\n",
    "    return DotProductAttention(q, k, v, mask, scale=scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

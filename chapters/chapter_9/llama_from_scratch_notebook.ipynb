{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_cap = torch.cuda.get_device_capability()\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"\n",
    "torch.cuda.set_device(device)\n",
    "torch.manual_seed(8855)\n",
    "print(torch.__version__)\n",
    "print(device, device_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2 from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Huggingface\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama2/\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "# tokenizer.pad_token = tokenizer.eos_token #Optional\n",
    "\n",
    "vocab = tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example):\n",
    "    return tokenizer.encode(example, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def decode(example):\n",
    "    return tokenizer.batch_decode(\n",
    "        example,\n",
    "        skip_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0]\n",
    "\n",
    "\n",
    "print(f\"Vocab Size: {len(vocab)}\")\n",
    "decode(\n",
    "    encode(\n",
    "        \"hello I am a specifically designed long sentence to make sure this is working not only adequately, but good enough for our batch functions\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_CONFIG = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"batch_size\": 16,\n",
    "    \"context_window\": 32,\n",
    "    \"d_model\": 288,\n",
    "    \"hidden_dim\": 768,\n",
    "    \"epochs\": 1000,\n",
    "    \"log_interval\": 50,\n",
    "    \"n_heads\": 6,\n",
    "    \"n_layers\": 6,\n",
    "}\n",
    "GLOBAL_KEEP_TRACK = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Data only\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": [\"../../data/TinyStoriesv1andv2-train.txt\"],\n",
    "        \"val\": [\"../../data/TinyStoriesv1andv2-valid.txt\"],\n",
    "    },\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Stream Data\n",
    "# dataset = load_dataset('IMJONEZZ/CombinedTinyStories') # optional: streaming=True but to_iterable is faster\n",
    "# dataset = dataset.to_iterable_dataset()\n",
    "\n",
    "# Minimal preprocessing\n",
    "clean_dataset = dataset.filter(lambda example: len(example[\"text\"]) > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short story. Possible Story: \"\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "encoded_dataset = clean_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        [prompt + x for x in examples[\"text\"]],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "train_data = iter(encoded_dataset[\"train\"].shuffle())\n",
    "val_data = iter(encoded_dataset[\"val\"].shuffle())\n",
    "\n",
    "test = next(train_data)\n",
    "print(f\"Actual text: {test['text']}\")\n",
    "print(f\"Input Ids: {tokenizer.decode(test['input_ids'])}\")\n",
    "print(f\"Length of text: {len(test['input_ids'])}\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #For non-Windows users\n",
    "def get_batches(\n",
    "    data,\n",
    "    batch_size,\n",
    "    context_window,\n",
    "    config=MASTER_CONFIG,\n",
    "    debug=False,\n",
    "):\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(\n",
    "        batch_size\n",
    "    ):  # Adjust this lower if you're running out of memory\n",
    "        batch_data = next(data)\n",
    "\n",
    "        # pick random starting points\n",
    "        ix = torch.randint(\n",
    "            0, len(batch_data[\"input_ids\"]) - context_window - 1, (2,)\n",
    "        )\n",
    "        batch_x = torch.stack(\n",
    "            [batch_data[\"input_ids\"][i : i + context_window] for i in ix]\n",
    "        ).long()\n",
    "        batch_y = torch.stack(\n",
    "            [\n",
    "                batch_data[\"input_ids\"][i + 1 : i + context_window + 1]\n",
    "                for i in ix\n",
    "            ]\n",
    "        ).long()\n",
    "        x.append(batch_x)\n",
    "        y.append(batch_y)\n",
    "    x = torch.cat((x), 0).to(device)\n",
    "    y = torch.cat((y), 0).to(device)\n",
    "    if debug:\n",
    "        print(f\"ix: {ix}\\nx: {x}\\ny: {y}\")\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_lora_batches(\n",
    "    data,\n",
    "    batch_size,\n",
    "    context_window=32,\n",
    "):\n",
    "    x = []\n",
    "    y = []\n",
    "    for _ in range(batch_size // 2):\n",
    "        x_data = next(data)\n",
    "        y_data = next(data)\n",
    "\n",
    "        x_data = torch.stack([x_data[\"input_ids\"]]).long()\n",
    "        y_data = torch.stack([y_data[\"input_ids\"]]).long()\n",
    "\n",
    "        x.append(x_data)\n",
    "        y.append(y_data)\n",
    "    x = torch.cat((x), 0).to(device)\n",
    "    y = torch.cat((y), 0).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_loss(model, lora=False, config=MASTER_CONFIG):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for name, split in zip([\"train\", \"val\"], [train_data, val_data]):\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            if lora == True:\n",
    "                xb, yb = get_lora_batches(split, config[\"batch_size\"])\n",
    "            else:\n",
    "                xb, yb = get_batches(\n",
    "                    split,\n",
    "                    config[\"batch_size\"],\n",
    "                    config[\"context_window\"],\n",
    "                )\n",
    "            _, loss = model(xb, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[name] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model,\n",
    "    config=MASTER_CONFIG,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    max_new_tokens=30,\n",
    "):\n",
    "    idx_list = [tokenized_prompt] * 5\n",
    "    idx = torch.cat((idx_list), 0).long().to(device)\n",
    "    # idx = torch.ones(5, 1).long().to(device) #Alternative without prompt\n",
    "    for _ in range(max_new_tokens):\n",
    "        # call the model\n",
    "        logits = model(idx[:, -config[\"context_window\"] :])\n",
    "        last_time_step_logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # all the batches (1), last time step, all the logits\n",
    "\n",
    "        last_time_step_logits = last_time_step_logits / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(\n",
    "                last_time_step_logits,\n",
    "                min(top_k, last_time_step_logits.size(-1)),\n",
    "            )\n",
    "            last_time_step_logits[\n",
    "                last_time_step_logits < v[:, [-1]]\n",
    "            ] = -float(\"Inf\")\n",
    "        p = F.softmax(\n",
    "            last_time_step_logits, dim=-1\n",
    "        )  # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )  # sample from the distribution to get the next token\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
    "    return [tokenizer.decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gross neural network that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForwardNN(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"d_model\"], config[\"vocab_size\"]),\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "model = SimpleFeedForwardNN(MASTER_CONFIG).to(device)\n",
    "# opt_model = torch.compile(model) #Again, non-Windows folks should be compiling functions and models\n",
    "# xs, ys = get_batches(\n",
    "#     train_data,\n",
    "#     MASTER_CONFIG[\"batch_size\"],\n",
    "#     MASTER_CONFIG[\"context_window\"],\n",
    "# )\n",
    "\n",
    "# logits, loss = model(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    data=None,\n",
    "    config=MASTER_CONFIG,\n",
    "    lora=False,\n",
    "    print_logs=False,\n",
    "):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if lora:\n",
    "                xs, ys = get_lora_batches(data, config[\"batch_size\"])\n",
    "            else:\n",
    "                xs, ys = get_batches(\n",
    "                    data, config[\"batch_size\"], config[\"context_window\"]\n",
    "                )\n",
    "\n",
    "            logits, loss = model(xs, targets=ys)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "            if epoch % config[\"log_interval\"] == 0:\n",
    "                batch_time = time.time() - start_time\n",
    "                x = get_loss(model, lora=lora)\n",
    "                losses += [x]\n",
    "                if print_logs:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch} | train loss {x['train']:.3f} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA: {timedelta(seconds=(batch_time * (config['epochs'] - epoch)/config['log_interval']))}\"\n",
    "                    )\n",
    "                start_time = time.time()\n",
    "\n",
    "                if scheduler:\n",
    "                    print(\"lr: \", scheduler.get_last_lr())\n",
    "        except StopIteration:\n",
    "            print(f\"Reached end of dataset on step {epoch}\")\n",
    "            break\n",
    "\n",
    "    GLOBAL_KEEP_TRACK.append(\n",
    "        f\"{type(model).__name__} {sum([m.numel() for m in model.parameters()])} Params | Train: {losses[-1]['train']:.3f} | Val: {losses[-1]['val']:.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"training loss {losses[-1]['train']:.3f} | validation loss: {losses[-1]['val']:.3f}\"\n",
    "    )\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    ")\n",
    "train(model, optimizer, data=train_data, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization\n",
    "\n",
    "This is as opposed to the original Batch Normalization used in the original transformers paper.\n",
    "Basically it works by taking 1/sqrt(N) * frobenius_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNormalization(nn.Module):\n",
    "    def __init__(self, layer_shape, eps=1e-5, bias=False):\n",
    "        super(RMSNormalization, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.register_parameter(\n",
    "            \"scale\", nn.Parameter(torch.ones(layer_shape))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        assumes shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        ff_rms = torch.linalg.norm(x, dim=(1, 2)) * x[0].numel() ** -0.5\n",
    "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
    "        return self.scale[: x.shape[1], :].unsqueeze(0) * raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForwardNN_RMS(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.rms = RMSNormalization(\n",
    "            (config[\"context_window\"], config[\"d_model\"])\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config[\"d_model\"], config[\"vocab_size\"]),\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        x = self.rms(x)  # rms pre-normalization\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "model = SimpleFeedForwardNN_RMS(MASTER_CONFIG).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "train(model, optimizer, data=train_data, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE scaling\n",
    "\n",
    "Rotary Embeddings changes the positional encoding from being based on sine and cosine to being based on the rotation of the embeddings, with a different rotation at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(\n",
    "    nopython=False\n",
    ")  # Careful running @jit and @njit in a jupyter notebook, nopython compilation might fail.\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros(\n",
    "        (context_window, embedding_dim, embedding_dim), requires_grad=False\n",
    "    )\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim // 2):\n",
    "            theta = 10000.0 ** (-2.0 * (i - 1) / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "K = 3\n",
    "config = {\n",
    "    \"batch_size\": 10,\n",
    "    \"d_model\": 32,\n",
    "    \"n_heads\": 8,\n",
    "    \"context_window\": K**2,\n",
    "}\n",
    "batch = torch.randn(1, config[\"context_window\"], config[\"d_model\"])\n",
    "R = get_rotary_matrix(config[\"context_window\"], config[\"d_model\"])\n",
    "# clear_output()\n",
    "fig, ax = plt.subplots(K, K, figsize=(K * 3, K * 4))\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        ax[i, j].imshow(R[i * K + j, :, :].detach().numpy())\n",
    "        ax[i, j].set_title(f\"rotation at {i * K + j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 10,\n",
    "    \"d_model\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"context_window\": 16,\n",
    "}\n",
    "\n",
    "\n",
    "class RoPEMaskedAttentionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.w_q = nn.Linear(\n",
    "            config[\"d_model\"], config[\"d_model\"], bias=False\n",
    "        )\n",
    "        self.w_k = nn.Linear(\n",
    "            config[\"d_model\"], config[\"d_model\"], bias=False\n",
    "        )\n",
    "        self.w_v = nn.Linear(\n",
    "            config[\"d_model\"], config[\"d_model\"], bias=False\n",
    "        )\n",
    "\n",
    "        self.R = get_rotary_matrix(\n",
    "            config[\"context_window\"], config[\"d_model\"]\n",
    "        ).to(device)\n",
    "\n",
    "    @jit(nopython=False)\n",
    "    def get_rotary_matrix(context_window, embedding_dim):\n",
    "        R = torch.zeros(\n",
    "            (context_window, embedding_dim, embedding_dim),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        for position in range(context_window):\n",
    "            for i in range(embedding_dim // 2):\n",
    "                theta = 10000.0 ** (-2.0 * (i - 1) / embedding_dim)\n",
    "                m_theta = position * theta\n",
    "                R[position, 2 * i, 2 * i] = np.cos(m_theta)\n",
    "                R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)\n",
    "                R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)\n",
    "                R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)\n",
    "        return R\n",
    "\n",
    "    def forward(self, x, return_attn_weights=False):\n",
    "        b, m, d = x.shape\n",
    "\n",
    "        q = self.w_q(x).to(device)\n",
    "        k = self.w_k(x).to(device)\n",
    "        v = self.w_v(x).to(device)\n",
    "\n",
    "        q_rotated = (\n",
    "            (torch.bmm(q.transpose(0, 1), self.R[:m]))\n",
    "            .transpose(0, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        k_rotated = (\n",
    "            (torch.bmm(k.transpose(0, 1), self.R[:m]))\n",
    "            .transpose(0, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
    "        )\n",
    "\n",
    "        if return_attn_weights:\n",
    "            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0).to(\n",
    "                device\n",
    "            )\n",
    "            attn_weights = (\n",
    "                torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d)\n",
    "                + attn_mask\n",
    "            )\n",
    "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "            return activations, attn_weights\n",
    "        return activations\n",
    "\n",
    "\n",
    "layer = RoPEMaskedAttentionHead(config)\n",
    "batch = torch.randn(\n",
    "    (config[\"batch_size\"], config[\"context_window\"], config[\"d_model\"])\n",
    ")\n",
    "output, attn_weights = layer(batch, return_attn_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPEMaskedMultiheadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                RoPEMaskedAttentionHead(config).to(device)\n",
    "                for _ in range(config[\"n_heads\"])\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            config[\"n_heads\"] * config[\"d_model\"], config[\"d_model\"]\n",
    "        ).to(device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        heads = [h(x) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "layer = RoPEMaskedMultiheadAttention(MASTER_CONFIG)\n",
    "batch = torch.ones(\n",
    "    (\n",
    "        MASTER_CONFIG[\"batch_size\"],\n",
    "        MASTER_CONFIG[\"context_window\"],\n",
    "        MASTER_CONFIG[\"d_model\"],\n",
    "    )\n",
    ")\n",
    "output = layer(batch)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForwardNN_RMS_Rope(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.rms = RMSNormalization(\n",
    "            (config[\"context_window\"], config[\"d_model\"])\n",
    "        ).to(device)\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"d_model\"]), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.last_linear = nn.Linear(\n",
    "            config[\"d_model\"], config[\"vocab_size\"]\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.linear(x)\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "model = SimpleFeedForwardNN_RMS_Rope(MASTER_CONFIG).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "train(model, optimizer, data=train_data, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU\n",
    "\n",
    "Replace ReLU activation function. Defined as SwiGLU(x) = Swishβ(xW + b)⊗︀(xV + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "        self.beta = torch.randn(1, requires_grad=True)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.ones(1))\n",
    "        self.register_parameter(\"beta\", self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(\n",
    "            self.beta * self.linear_gate(x)\n",
    "        )\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForwardNN_RMS_RoPE_SwiGLU(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.rms = RMSNormalization(\n",
    "            (config[\"context_window\"], config[\"d_model\"])\n",
    "        )\n",
    "        self.rope_attention = RoPEMaskedMultiheadAttention(config)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
    "            SwiGLU(config[\"d_model\"]),\n",
    "        )\n",
    "\n",
    "        self.last_linear = nn.Linear(\n",
    "            config[\"d_model\"], config[\"vocab_size\"]\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.rope_attention(x)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.linear(x)\n",
    "\n",
    "        logits = self.last_linear(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "model = SimpleFeedForwardNN_RMS_RoPE_SwiGLU(MASTER_CONFIG).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "train(model, optimizer, data=train_data, print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama\n",
    "\n",
    "So what makes Llama 2 different from regular simple feed forward networks? Blocks of RMSNorm and Residual Rope Attention. Let's make those blocks and create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNormalization(\n",
    "            (config[\"context_window\"], config[\"d_model\"])\n",
    "        ).to(device)\n",
    "\n",
    "        self.attention = RoPEMaskedMultiheadAttention(config).to(device)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"hidden_dim\"]),\n",
    "            SwiGLU(config[\"hidden_dim\"]),\n",
    "            nn.Linear(config[\"hidden_dim\"], config[\"d_model\"]),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rms(x)\n",
    "        x = x + self.attention(x)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLlama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (f\"llama_{i}\", LlamaBlock(config))\n",
    "                    for i in range(config[\"n_layers\"])\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n",
    "            SwiGLU(config[\"d_model\"]),\n",
    "            nn.Linear(config[\"d_model\"], config[\"vocab_size\"]),\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        x = self.llama_blocks(x)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        else:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_CONFIG[\"epochs\"] = 1000\n",
    "\n",
    "llama = SimpleLlama(MASTER_CONFIG).to(device)\n",
    "optimizer = torch.optim.AdamW(llama.parameters())\n",
    "train(\n",
    "    llama, optimizer, data=train_data, config=MASTER_CONFIG, print_logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    llama,\n",
    "    config=MASTER_CONFIG,\n",
    "    temperature=1.0,\n",
    "    top_k=25,\n",
    "    max_new_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_CONFIG[\"epochs\"] = 1000\n",
    "MASTER_CONFIG[\"batch_size\"] = 16\n",
    "MASTER_CONFIG[\"d_model\"] = 768\n",
    "MASTER_CONFIG[\"n_layers\"] = 8\n",
    "MASTER_CONFIG[\"context_window\"] = 64\n",
    "\n",
    "llama = SimpleLlama(MASTER_CONFIG).to(device)\n",
    "\n",
    "llama_optimizer = torch.optim.AdamW(\n",
    "    llama.parameters(),\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=1e-1,\n",
    "    eps=1e-9,\n",
    "    lr=5e-4,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    llama_optimizer, 1000, eta_min=1e-5\n",
    ")\n",
    "train_data = iter(encoded_dataset[\"train\"].shuffle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    llama,\n",
    "    llama_optimizer,\n",
    "    scheduler=scheduler,\n",
    "    data=train_data,\n",
    "    config=MASTER_CONFIG,\n",
    "    print_logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\n",
    "    llama,\n",
    "    config=MASTER_CONFIG,\n",
    "    temperature=1.0,\n",
    "    top_k=100,\n",
    "    max_new_tokens=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in GLOBAL_KEEP_TRACK:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA_CONFIG = {\n",
    "#     \"vocab_size\": 32000,\n",
    "#     \"batch_size\": 2,\n",
    "#     \"context_window\": 256,\n",
    "#     \"d_model\": 768,\n",
    "#     \"hidden_dim\": 1536,\n",
    "#     \"epochs\": 1,\n",
    "#     \"log_interval\": 1,\n",
    "#     \"n_heads\": 8,\n",
    "#     \"n_layers\": 8,\n",
    "# }\n",
    "\n",
    "# # Llama2 7b config:\n",
    "# # LLAMA_2_7b_CONFIG = {\n",
    "# #     \"hidden_act\": \"silu\",\n",
    "# #     \"hidden_dim\": 4096,\n",
    "# #     \"initializer_range\": 0.02,\n",
    "# #     \"swiglu_size\": 11008,\n",
    "# #     \"context_window\": 4096,\n",
    "# #     \"d_model\": 2048,\n",
    "# #     \"n_heads\": 32,\n",
    "# #     \"n_layers\": 32,\n",
    "# #     \"num_key_value_heads\": 32,\n",
    "# #     \"rms_norm_eps\": 1e-05,\n",
    "# #     \"torch_dtype\": \"float16\", # Ours is float32 right now\n",
    "# #     \"vocab_size\": 32000\n",
    "# # }\n",
    "\n",
    "# # # Llama2 70b config:\n",
    "# # LLAMA_2_70b_CONFIG = {\n",
    "# #     \"hidden_act\": \"silu\",\n",
    "# #     \"hidden_dim\": 8192,\n",
    "# #     \"initializer_range\": 0.02,\n",
    "# #     \"swiglu_size\": 28672,\n",
    "# #     \"context_window\": 4096,\n",
    "# #     \"d_model\": 2048,\n",
    "# #     \"n_heads\": 64,\n",
    "# #     \"n_layers\": 80,\n",
    "# #     \"num_key_value_heads\": 8,\n",
    "# #     \"rms_norm_eps\": 1e-05,\n",
    "# #     \"torch_dtype\": \"float16\", # Ours is float32 right now\n",
    "# #     \"vocab_size\": 32000\n",
    "# #     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with epochs instead of steps\n",
    "# def train(\n",
    "#     model, optimizer, dataset, scheduler=None, config=LLAMA_CONFIG, print_logs=False\n",
    "# ):\n",
    "#     losses = []\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     for epoch in range(config[\"epochs\"]):\n",
    "#         for step, example in enumerate(dataset):\n",
    "#             try:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             xs, ys = _get_batch(example, 2, config['context_window'])\n",
    "#             _, loss = model(xs, targets=ys)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             if scheduler:\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             if i % config[\"log_interval\"] == 0:\n",
    "#                 batch_time = time.time() - start_time\n",
    "#                 x = get_loss(model)\n",
    "#                 losses += [x]\n",
    "#                 if print_logs:\n",
    "#                     print(\n",
    "#                         f\"Epoch {epoch} | Step {i} | train loss {x['train']:.3f} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds: {batch_time * (config['epochs'] - i) :.3f}\"\n",
    "#                     )\n",
    "#                 start_time = time.time()\n",
    "\n",
    "#                 if scheduler:\n",
    "#                     print(\"lr: \", scheduler.get_last_lr())\n",
    "#               except StopIteration:\n",
    "#                   print(f\"Finished dataset at step {step}\")\n",
    "#                   break\n",
    "#     print(f\"training loss {losses[-1]['train']} | validation loss: {losses[-1]['val']}\")\n",
    "#     GLOBAL_KEEP_TRACK.append(f\"{type(model).__name__} {sum([m.numel() for m in model.parameters()])} Params | Train: {losses[-1]['train']} | Val: {losses[-1]['val']}\")\n",
    "#     return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama = Llama(LLAMA_CONFIG).to(device)\n",
    "\n",
    "# llama_optimizer = torch.optim.Adam(\n",
    "#     llama.parameters(),\n",
    "#     betas=(0.9, 0.95),\n",
    "#     weight_decay=0.1,\n",
    "#     eps=1e-9,\n",
    "#     lr=1e-3,\n",
    "# )\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     llama_optimizer, 1000, eta_min=1e-5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.to(\"cpu\")\n",
    "qconfig_dict = {\n",
    "    torch.nn.Embedding: torch.quantization.float_qparams_weight_only_qconfig,\n",
    "    torch.nn.Linear: torch.quantization.default_dynamic_qconfig,\n",
    "}\n",
    "# Post Training Dynamic Quantization\n",
    "dynamic_quantized_llama = torch.quantization.quantize_dynamic(\n",
    "    llama, qconfig_dict, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "\n",
    "# Get Size difference\n",
    "def get_param_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    return param_size\n",
    "\n",
    "\n",
    "def get_buffer_size(model):\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    return buffer_size\n",
    "\n",
    "\n",
    "def get_param_and_buffer_size(model):\n",
    "    param_size = get_param_size(model)\n",
    "    buffer_size = get_buffer_size(model)\n",
    "    return param_size, buffer_size\n",
    "\n",
    "\n",
    "def get_size_difference(models: list) -> str:\n",
    "    keeping_track = []\n",
    "    for idx, model in enumerate(models):\n",
    "        param_size, buffer_size = get_param_and_buffer_size(model)\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        keeping_track.append(\n",
    "            f\"{type(model).__name__} size: {size_all_mb:.3f}MB\"\n",
    "        )\n",
    "    return keeping_track\n",
    "\n",
    "\n",
    "list_of_sizes = get_size_difference([llama, dynamic_quantized_llama])\n",
    "for size in list_of_sizes:\n",
    "    print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA\n",
    "\n",
    "If you have this:\n",
    "\n",
    "<p>def forward(self, x):<br>\n",
    "    x = self.linear_1(x)<br>\n",
    "    x = F.relu(x)<br>\n",
    "    x = self.linear_2(x)<br>\n",
    "    return x<br></p>\n",
    "\n",
    "change it to this:\n",
    "\n",
    "<p>def forward(self, x):<br>\n",
    "    x = self.linear_1(x) + self.lora_1(x)<br>\n",
    "    x = F.relu(x)<br>\n",
    "    x = self.linear_2(x) + self.lora_2(x)<br>\n",
    "    return logits<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        standard_deviation = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(\n",
    "            torch.randn(in_dim, rank) * standard_deviation\n",
    "        )\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "# Utils\n",
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fan_in,\n",
    "        fan_out,\n",
    "        fan_in_fan_out=False,\n",
    "        rank=0,\n",
    "        lora_dropout_p=0.0,\n",
    "        lora_alpha=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.swap = (\n",
    "            (lambda x: (x[1], x[0])) if fan_in_fan_out else (lambda x: x)\n",
    "        )\n",
    "        self.lora_A = nn.Parameter(torch.zeros(self.swap((rank, fan_in))))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.swap((fan_out, rank))))\n",
    "        self.lora_alpha, self.rank = lora_alpha, rank\n",
    "        self.scaling = lora_alpha / rank\n",
    "        self.lora_dropout = (\n",
    "            nn.Dropout(p=lora_dropout_p)\n",
    "            if lora_dropout_p > 0\n",
    "            else lambda x: x\n",
    "        )\n",
    "        self.dropout_fn = (\n",
    "            self._dropout if lora_dropout_p > 0 else lambda x: x\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"lora_dropout_mask\",\n",
    "            torch.ones(self.swap((1, fan_in)), dtype=self.lora_A.dtype),\n",
    "        )\n",
    "        self.forward_fn = self.lora_forward\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.forward_fn(X)\n",
    "\n",
    "    def lora_forward(self, X):\n",
    "        return (\n",
    "            X\n",
    "            + torch.matmul(\n",
    "                *self.swap((self.lora_B, self.dropout_fn(self.lora_A)))\n",
    "            ).view(X.shape)\n",
    "            * self.scaling\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def lora_from_layer(\n",
    "        cls, layer, rank=0, lora_dropout_p=0.0, lora_alpha=1\n",
    "    ):\n",
    "        fan_out, fan_in = layer.weight.shape\n",
    "        return cls(\n",
    "            fan_in,\n",
    "            fan_out,\n",
    "            fan_in_fan_out=False,\n",
    "            rank=rank,\n",
    "            lora_dropout_p=lora_dropout_p,\n",
    "            lora_alpha=lora_alpha,\n",
    "        )\n",
    "\n",
    "\n",
    "lora_config = {\n",
    "    nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.lora_from_layer, rank=16),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def apply_lora(layer, register=True, merge=False, lora_config=lora_config):\n",
    "    if register:\n",
    "        if type(layer) in lora_config:\n",
    "            for attr_name, parametrization in lora_config[\n",
    "                type(layer)\n",
    "            ].items():\n",
    "                torch.nn.utils.parametrize.register_parametrization(\n",
    "                    layer, attr_name, parametrization(layer)\n",
    "                )\n",
    "    else:\n",
    "        if hasattr(layer, \"parametrizations\"):\n",
    "            for attr_name in layer.parametrizations.keys():\n",
    "                torch.nn.utils.parametrize.remove_parametrizations(\n",
    "                    layer, attr_name, leave_parametrized=merge\n",
    "                )\n",
    "\n",
    "\n",
    "def add_lora(model, lora_config=lora_config):\n",
    "    model.apply(partial(apply_lora, lora_config=lora_config))\n",
    "\n",
    "\n",
    "def merge_lora(model):\n",
    "    model.apply(partial(apply_lora, register=False, merge=True))\n",
    "\n",
    "\n",
    "def name_is_lora(name):\n",
    "    return (\n",
    "        len(name.split(\".\")) >= 4\n",
    "        and (name.split(\".\")[-4]) == \"parametrizations\"\n",
    "        and name.split(\".\")[-1] in [\"lora_A\", \"lora_B\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "\n",
    "def get_lora_params(model, print_shapes=False):\n",
    "    return get_params_by_name(\n",
    "        model, print_shapes=print_shapes, name_filter=name_is_lora\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how the blocks change, but we'll use our already-trained model\n",
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNormalization(\n",
    "            (config[\"context_window\"], config[\"d_model\"])\n",
    "        ).to(device)\n",
    "\n",
    "        self.attention = RoPEMaskedMultiheadAttention(config).to(device)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            LinearWithLoRA(config[\"d_model\"], config[\"d_model\"]),  # NEW\n",
    "            SwiGLU(config[\"d_model\"]),\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rms(x)\n",
    "        x = x + self.attention(x)\n",
    "\n",
    "        x = self.rms(x)\n",
    "        x = x + self.feedforward(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleLlama(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            config[\"vocab_size\"], config[\"d_model\"]\n",
    "        )\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (f\"llama_{i}\", LlamaBlock(config))\n",
    "                    for i in range(config[\"n_layers\"])\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            LinearWithLoRA(config[\"d_model\"], config[\"d_model\"]),  # NEW\n",
    "            SwiGLU(config[\"d_model\"]),\n",
    "            LinearWithLoRA(config[\"d_model\"], config[\"vocab_size\"]),  # NEW\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n",
    "        )\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        x = self.llama_blocks(x)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "\n",
    "        else:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, self.config[\"vocab_size\"]),\n",
    "                targets.view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id,\n",
    "                # reduction=\"sum\",\n",
    "            )\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Dataset for Lora\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": [\"../../data/Lima-train.csv\"],\n",
    "        \"val\": [\"../../data/Lima-test.csv\"],\n",
    "    },\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "encoded_dataset = dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ),\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Add LoRA to trained model\n",
    "llama.to(\"cpu\")\n",
    "add_lora(llama)\n",
    "llama.to(device)\n",
    "\n",
    "# Step 2: Get the LoRA params instead of the whole model's\n",
    "parameters = [{\"params\": list(get_lora_params(llama))}]\n",
    "# Step 3: initialize optimizer with LoRA Params\n",
    "lora_optimizer = torch.optim.AdamW(parameters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train\n",
    "for _ in range(3):\n",
    "    train_data = iter(encoded_dataset[\"train\"])\n",
    "    val_data = iter(encoded_dataset[\"val\"])\n",
    "    train(\n",
    "        llama,\n",
    "        lora_optimizer,\n",
    "        scheduler,\n",
    "        data=train_data,\n",
    "        config=MASTER_CONFIG,\n",
    "        lora=True,\n",
    "        print_logs=True,\n",
    "    )\n",
    "\n",
    "# Step 5: export the params\n",
    "state_dict = llama.state_dict()\n",
    "lora_state_dict = {k: v for k, v in state_dict.items() if name_is_lora(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Inferencing with LoRA\n",
    "add_lora(llama)\n",
    "\n",
    "_ = llama.load_state_dict(lora_state_dict, strict=False)\n",
    "\n",
    "merge_lora(llama)\n",
    "\n",
    "generate(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(llama, \"./llama2/llama.pth\")\n",
    "torch.save(lora_state_dict, \"./llama2/lora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio app that looks like this:\n",
    "\n",
    "# # Space will need your token to request hardware: set it as a Secret !\n",
    "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# # Space own repo_id\n",
    "# TRAINING_SPACE_ID = \"your_username/your_repo\"\n",
    "\n",
    "# from huggingface_hub import HfApi, SpaceHardware\n",
    "# api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "# # On Space startup, check if a task is scheduled. If yes, finetune the model. If not,\n",
    "# # display an interface to request a new task.\n",
    "# task = get_task()\n",
    "# if task is None:\n",
    "#     # Start Gradio app\n",
    "#     def gradio_fn(task):\n",
    "#         # On user request, add task and request hardware\n",
    "#         add_task(task)\n",
    "#         api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.CPU_BASIC)\n",
    "\n",
    "#     gr.Interface(fn=gradio_fn, ...).launch()\n",
    "# else:\n",
    "#     runtime = api.get_space_runtime(repo_id=TRAINING_SPACE_ID)\n",
    "#     # Check if Space is loaded with a GPU.\n",
    "#     if runtime.hardware == SpaceHardware.CPU_BASIC:\n",
    "#         # If yes, finetune base model on dataset !\n",
    "#         chat_with_user(task)\n",
    "\n",
    "#         # Then, mark the task as \"DONE\"\n",
    "#         mark_as_done(task)\n",
    "\n",
    "#         # DO NOT FORGET: set back CPU hardware\n",
    "#         api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.CPU_BASIC)\n",
    "#     else:\n",
    "#         api.request_space_hardware(repo_id=TRAINING_SPACE_ID, hardware=SpaceHardware.CPU_BASIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub -q\n",
    "\n",
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.create_repo(\n",
    "    repo_id=\"your_username/your_repo\", repo_type=\"space\", space_sdk=\"gradio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_save = [\n",
    "    \"llama.pth\",\n",
    "    \"lora.pth\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer.model\",\n",
    "    \"gradio_app.py\",\n",
    "]\n",
    "for thing in stuff_to_save:\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=f\"./llama2/{thing}\",\n",
    "        path_in_repo=thing,\n",
    "        repo_id=\"your_username/your_repo\",\n",
    "        repo_type=\"space\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
